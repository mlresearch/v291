---
title: Towards Fundamental Limits for Active Multi-distribution Learning
section: Original Papers
abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
  (PAC) learning to the setting in which a family of $k$ distributions, $\{D_i\}_{i\in[k]}$,
  is considered and a classifierâ€™s performance is measured by its error under the
  worst distribution. This problem has attracted a lot of recent interests due to
  its  applications in collaborative learning, fairness, and robustness. Despite a
  rather complete picture of sample complexity of passive multi-distribution learning,
  research on active multi-distribution learning remains scarce,  with algorithms
  whose optimality remaining unknown. In this paper, we develop new algorithms for
  active multi-distribution learning and establish improved label complexity upper
  and lower bounds, in distribution-dependent and distribution-free settings. Specifically,
  in the near-realizable setting we prove an upper bound of  $\widetilde{O}\Bigl(\theta_{\mathrm{max}}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$
  and $\widetilde{O}\Bigl(\theta_{\mathrm{max}}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
  in the realizable and agnostic settings respectively, where $\theta_{\mathrm{max}}$
  is the maximum disagreement coefficient among the $k$ distributions, $d$ is the
  VC dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
  best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we show
  that the bound in the realizable setting is information-theoretically optimal and
  that the $k\nu/\varepsilon^2$ term in the agnostic setting is fundamental for proper
  learners. We also establish instance-dependent sample complexity bound for passive
  multidistribution learning that smoothly interpolates between realizable and agnostic
  regimes (Blum et al., 2017; Zhang et al., 2024), which may be of independent interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang25c
month: 0
tex_title: Towards Fundamental Limits for Active Multi-distribution Learning
firstpage: 6041
lastpage: 6090
page: 6041-6090
order: 6041
cycles: false
bibtex_author: Zhang, Chicheng and Zhou, Yihan
author:
- given: Chicheng
  family: Zhang
- given: Yihan
  family: Zhou
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/zhang25c/zhang25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
