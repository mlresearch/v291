---
title: Optimistically Optimistic Exploration for Provably Efficient Infinite-Horizon
  Reinforcement and Imitation Learning
section: Original Papers
abstract: 'We study the problem of reinforcement learning in infinite-horizon discounted
  linear Markov decision processes (MDPs), and propose the first computationally efficient
  algorithm achieving rate-optimal regret guarantees in this setting. Our main idea
  is to combine two classic techniques for optimistic exploration: additive exploration
  bonuses applied to the reward function, and artificial transitions made to an absorbing
  state with maximal return. We show that, combined with a regularized approximate
  dynamic-programming scheme, the resulting algorithm achieves a regret of order $\tilde{\mathcal{O}}
  (\sqrt{d^3 (1 - \gamma)^{- 7 / 2} T})$, where $T$ is the total number of sample
  transitions, $\gamma \in (0,1)$ is the discount factor, and $d$ is the feature dimensionality.
  The results continue to hold against adversarial reward sequences, enabling application
  of our method to the problem of imitation learning in linear MDPs, where we achieve
  state-of-the-art results. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: moulin25a
month: 0
tex_title: Optimistically Optimistic Exploration for Provably Efficient Infinite-Horizon
  Reinforcement and Imitation Learning
firstpage: 4203
lastpage: 4270
page: 4203-4270
order: 4203
cycles: false
bibtex_author: Moulin, Antoine and Neu, Gergely and Viano, Luca
author:
- given: Antoine
  family: Moulin
- given: Gergely
  family: Neu
- given: Luca
  family: Viano
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/moulin25a/moulin25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
