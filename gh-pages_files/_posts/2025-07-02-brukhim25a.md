---
title: On the Hardness of Bandit Learning
section: Original Papers
abstract: 'We study the task of bandit learning, also known as best-arm identification,
  under the assumption that the true reward function f belongs to a known,  but arbitrary,
  function class F.   While many instances of this problem are well understood, we
  seek a general theory of bandit learnability, akin to the PAC framework for classification.
  Our investigation is guided by the following two fundamental questions: (1) which
  classes F are learnable,  and (2) how they are learnable. For example, in the case
  of binary PAC classification,  learnability is fully determined by a combinatorial
  dimension, namely, the VC dimension, and can be attained via a simple algorithmic
  principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic
  results, our findings reveal  fundamental limitations of learning in structured
  bandits, offering insights into the boundaries of bandit learnability. First, for
  the question of "which",  we show that the paradigm of identifying the learnable
  classes via a dimension-like quantity fails for bandit learning. We give a simple
  proof demonstrating that no combinatorial dimension can characterize bandit learnability,
  even in finite classes, following a standard definition of dimension introduced
  by Ben-David et al. (2019).  For the question of "how", we prove a computational
  hardness result: we construct a reward function class for which at most two queries
  are needed to find the optimal action, yet no algorithm can do so in polynomial
  time unless RP=NP. We also prove that this class admits efficient algorithms for
  standard (albeit possibly computationally hard) algorithmic operations often considered
  in learning theory, such as an ERM. This implies  that computational hardness is
  in this case inherent to the task of bandit learning. Beyond these results, we investigate
  additional themes such as learning under noise, trade-offs between noise models,
  and the relationship between query complexity and regret minimization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: brukhim25a
month: 0
tex_title: On the Hardness of Bandit Learning
firstpage: 4452
lastpage: 4485
page: 4452-4485
order: 4452
cycles: false
bibtex_author: Brukhim, Nataly and Pacchiano, Aldo and Dudik, Miroslav and Schapire,
  Robert
author:
- given: Nataly
  family: Brukhim
- given: Aldo
  family: Pacchiano
- given: Miroslav
  family: Dudik
- given: Robert
  family: Schapire
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/brukhim25a/brukhim25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
