---
title: Learning Compositional Functions with Transformers from Easy-to-Hard Data
section: Original Papers
abstract: 'Transformer-based language models have demonstrated impressive capabilities
  across a range of complex reasoning tasks. Prior theoretical work exploring the
  expressive power of transformers has shown that they can efficiently perform multi-step
  reasoning tasks involving parallelizable computations. However, the learnability
  of such constructions, particularly the conditions on the data distribution that
  enable efficient learning via SGD, remains an open question. Towards answering this
  question, we study the learnability of a task called the \emph{$k$-fold composition},
  which requires computing an interleaved composition of $k$ input permutations and
  $k$ hidden permutations, and can be expressed by a transformer with $O(\log k)$
  layers. On the negative front, we provide a Statistical Query lower bound showing
  that any learner which is trained on samples from the $k$-fold composition task
  and makes polynomially many queries must have sample size exponential in $k$, thus
  establishing a statistical-computational gap. On the other hand, we show that this
  function class can be efficiently learned, with runtime and sample complexity polynomial
  in $k$, by gradient descent on an $O(\log k)$-depth transformer via two different
  curriculum learning strategies: one in which data consists of $k’$-fold composition
  functions with $k’ \le k$ presented in increasing order of difficulty, and another
  in which all data is presented simultaneously. Our work sheds light on the necessity
  and sufficiency of having both easy and hard examples in the data distribution for
  transformers to learn complex compositional tasks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang25a
month: 0
tex_title: Learning Compositional Functions with Transformers from Easy-to-Hard Data
firstpage: 5632
lastpage: 5711
page: 5632-5711
order: 5632
cycles: false
bibtex_author: Wang, Zixuan and Nichani, Eshaan and Bietti, Alberto and Damian, Alex
  and Hsu, Daniel and Lee, Jason D and Wu, Denny
author:
- given: Zixuan
  family: Wang
- given: Eshaan
  family: Nichani
- given: Alberto
  family: Bietti
- given: Alex
  family: Damian
- given: Daniel
  family: Hsu
- given: Jason D
  family: Lee
- given: Denny
  family: Wu
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/wang25a/wang25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
