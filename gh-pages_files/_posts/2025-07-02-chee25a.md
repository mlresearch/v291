---
title: 'DiscQuant: A Quantization Method for Neural Networks Inspired by Discrepancy
  Theory'
section: Original Papers
abstract: " Quantizing the weights of a neural network has two steps: (1) Finding
  a good low bit-complexity representation for weights (which we call the quantization
  grid) and (2) Rounding the original weights to values in the quantization grid.
  In this paper, we study the problem of rounding optimally given any quantization
  grid. The simplest and most commonly used way to round is Round-to-Nearest (RTN).
  By rounding in a data-dependent way instead, one can improve the quality of the
  quantized model significantly. We study the rounding problem from the lens of \\emph{discrepancy
  theory}, which studies how well we can round a continuous solution to a discrete
  solution without affecting solution quality too much. We prove that given $m=\\poly\\left(\\frac{\\log
  n}{\\epsilon}\\right)$ samples from the data distribution, we can round nearly all
  $n$ model parameters such that the expected approximation error of the quantized
  model on the true data distribution is $\\le \\epsilon$ as long as the space of
  gradients of the original model is approximately low rank (which we empirically
  validate). Our algorithm is based on the famous Lovett-Meka algorithm from discrepancy
  theory and uses sticky Brownian motion to find a good rounding. We also give a simple
  and practical rounding algorithm called \\emph{DiscQuant}, which is inspired by
  our theoretical insights. In our experiments, we demonstrate that DiscQuant significantly
  improves over the prior state-of-the-art rounding method called GPTQ and the baseline
  RTN over a range of benchmarks on Phi3mini-3.8B and Llama3.1-8B. For example, rounding
  Phi3mini-3.8B to a fixed quantization grid with 3.25 bits per parameter using DiscQuant
  gets 64% accuracy on the GSM8k dataset, whereas GPTQ achieves 54% and RTN achieves
  31% (the original model achieves 84%). We make our code available at \\url{https://github.com/jerry-chee/DiscQuant}. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chee25a
month: 0
tex_title: 'DiscQuant: A Quantization Method for Neural Networks Inspired by Discrepancy
  Theory'
firstpage: 924
lastpage: 951
page: 924-951
order: 924
cycles: false
bibtex_author: Chee, Jerry and Backurs, Arturs and Heck, Rainie and Zhang, Li and
  Kulkarni, Janardhan and Rothvoss, Thomas and Gopi, Sivakanth
author:
- given: Jerry
  family: Chee
- given: Arturs
  family: Backurs
- given: Rainie
  family: Heck
- given: Li
  family: Zhang
- given: Janardhan
  family: Kulkarni
- given: Thomas
  family: Rothvoss
- given: Sivakanth
  family: Gopi
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/chee25a/chee25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
