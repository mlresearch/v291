---
title: The late-stage training dynamics of (stochastic) subgradient descent on homogeneous
  neural networks
section: Original Papers
abstract: We analyze the implicit bias of constant step stochastic subgradient descent
  (SGD). We consider the setting of binary classification with homogeneous neural
  networks â€“ a large class of deep neural networks with $\ReLU$-type activation functions
  such as MLPs and CNNs without biases. We interpret the dynamics of normalized SGD
  iterates as an Euler-like discretization of a conservative field flow that is naturally
  associated to the normalized classification margin. Owing to this interpretation,
  we show that normalized SGD iterates converge to the set of critical points of the
  normalized margin at late-stage training (i.e., assuming that the data is correctly
  classified with positive normalized margin).  Up to our knowledge, this is the first
  extension of the analysis of Lyu and Li (2020) on the discrete dynamics of gradient
  descent to the nonsmooth and stochastic setting. Our main result applies to binary
  classification with exponential or logistic losses. We additionally discuss extensions
  to more general settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: schechtman25a
month: 0
tex_title: The late-stage training dynamics of (stochastic) subgradient descent on
  homogeneous neural networks
firstpage: 5143
lastpage: 5172
page: 5143-5172
order: 5143
cycles: false
bibtex_author: Schechtman, Sholom and Schreuder, Nicolas
author:
- given: Sholom
  family: Schechtman
- given: Nicolas
  family: Schreuder
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/schechtman25a/schechtman25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
