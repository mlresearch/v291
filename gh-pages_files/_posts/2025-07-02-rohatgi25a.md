---
title: 'Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier:
  Autoregressive and Imitation Learning under Misspecification (extended abstract)'
section: Original Papers
abstract: 'Next-token prediction with the logarithmic loss is a cornerstone of autoregressive
  sequence modeling, but, in practice, suffers from \emph{error amplification}, where
  errors in the model compound and generation quality degrades as sequence length
  $H$ increases. From a theoretical perspective, this phenomenon should not appear
  in \emph{well-specified} settings, and, indeed, a growing body of empirical work
  hypothesizes that \emph{misspecification}, where the learner is not sufficiently
  expressive to represent the target distribution, may be the root cause. Under misspecification—where
  the goal is to learn as well as the best-in-class model up to a multiplicative approximation
  factor $C\geq{}1$—we confirm that $C$ indeed grows with $H$ for next-token prediction,
  lending theoretical support to this empirical hypothesis. We then ask whether this
  mode of error amplification is avoidable algorithmically, computationally, or information-theoretically,
  and uncover inherent computational-statistical tradeoffs. We show: \textbf{(1)}
  Information-theoretically, one can avoid error amplification and achieve $C=O(1)$.
  \textbf{(2)} Next-token prediction can be made robust to achieve $C=\tilde{O}(H)$,
  representing moderate error amplification, but this is an inherent barrier: \emph{any}
  next-token prediction-style objective must suffer $C=\Omega(H)$. \textbf{(3)} For
  the natural testbed of autoregressive \emph{linear} models, \emph{no computationally
  efficient algorithm} can achieve sub-polynomial approximation factor $C=e^{(\log
  H)^{1-\Omega(1)}}$; however, at least for binary token spaces, one can smoothly
  trade compute for statistical power and improve on $C=\Omega(H)$ in sub-exponential
  time. Our results have consequences in the more general setting of imitation learning,
  where the widely-used behavior cloning generalizes next-token prediction.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rohatgi25a
month: 0
tex_title: 'Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier:
  Autoregressive and Imitation Learning under Misspecification (extended abstract)'
firstpage: 4831
lastpage: 4837
page: 4831-4837
order: 4831
cycles: false
bibtex_author: Rohatgi, Dhruv and Block, Adam and Huang, Audrey and Krishnamurthy,
  Akshay and Foster, Dylan J.
author:
- given: Dhruv
  family: Rohatgi
- given: Adam
  family: Block
- given: Audrey
  family: Huang
- given: Akshay
  family: Krishnamurthy
- given: Dylan J.
  family: Foster
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/rohatgi25a/rohatgi25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
