---
title: Complexity of Injectivity and Verification of ReLU Neural Networks (Extended
  Abstract)
section: Original Papers
abstract: Neural networks with ReLU activation play a key role in modern machine learning.
  Understanding the functions represented by ReLU networks is a major topic in current
  research as this enables a better interpretability of learning processes.  Injectivity
  of a function computed by a ReLU network, that is, the question if different inputs
  to the network always lead to different outputs, plays a crucial role whenever invertibility
  of the function is required, such as, e.g., for inverse problems or generative models.
  The exact computational complexity of deciding injectivity was recently posed as
  an open problem (Puthawala et al. [JMLR 2022]). We answer this question by proving
  coNP-completeness. On the positive side, we show that the problem for a single ReLU-layer
  is still tractable for small input dimension; more precisely, we present a parameterized
  algorithm which yields fixed-parameter tractability with respect to the input dimension.  In
  addition, we study the network verification problem which is to verify that certain
  inputs only yield specific outputs. This is of great importance since neural networks
  are increasingly used in safety-critical systems. We prove that network verification
  is coNP-hard for a general class of input domains. Our results also exclude constant-factor
  polynomial-time approximations for the maximum of a function computed by a ReLU
  network. In this context, we also characterize surjectivity of functions computed
  by ReLU networks with one-dimensional output which turns out to be the complement
  of a basic network verification task.  We reveal interesting connections to computational
  convexity by formulating the surjectivity problem as a zonotope containment problem.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: froese25a
month: 0
tex_title: Complexity of Injectivity and Verification of ReLU Neural Networks (Extended
  Abstract)
firstpage: 2188
lastpage: 2189
page: 2188-2189
order: 2188
cycles: false
bibtex_author: Froese, Vincent and Grillo, Moritz and Skutella, Martin
author:
- given: Vincent
  family: Froese
- given: Moritz
  family: Grillo
- given: Martin
  family: Skutella
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/froese25a/froese25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
