---
title: Learning sparse generalized linear models with binary outcomes via iterative
  hard thresholding
section: Original Papers
abstract: In statistics, generalized linear models (GLMs) are widely used for modeling
  data and can expressively capture potential nonlinear dependence of the model’s
  outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes,
  which include logistic and probit regressions, are motivated by common tasks such
  as  binary classification with (possibly) non-separable data. In addition, in modern
  machine learning and statistics, data is often high-dimensional yet has a low intrinsic
  dimension, making sparsity constraints in models another reasonable consideration.
  In this work, we propose to use and analyze an iterative hard thresholding (projected
  gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding
  (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish
  that BIHT is statistically efficient and converges to the correct solution for parameter
  estimation in  a general class of sparse binary GLMs. Unlike many other methods
  for learning GLMs, including maximum likelihood estimation, generalized approximate
  message passing, and GLM-tron (Kakade et al., 2011; Bahmani et al., 2016), BIHT
  does not require knowledge of the GLM’s link function, offering flexibility and
  generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications,
  logistic and probit regression are additionally studied. In this regard, it is shown
  that in logistic regression, the algorithm is in fact statistically optimal in the
  sense that the order-wise sample complexity matches (up to logarithmic factors)
  the lower bound obtained previously. To the best of our knowledge, this is the first
  work achieving statistical optimality for logistic regression in all noise regimes
  with a computationally efficient algorithm. Moreover, for probit regression, our
  sample complexity is on the same order as that obtained for logistic regression.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: matsumoto25a
month: 0
tex_title: Learning sparse generalized linear models with binary outcomes via iterative
  hard thresholding
firstpage: 3933
lastpage: 4032
page: 3933-4032
order: 3933
cycles: false
bibtex_author: Matsumoto, Namiko and Mazumdar, Arya
author:
- given: Namiko
  family: Matsumoto
- given: Arya
  family: Mazumdar
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/matsumoto25a/matsumoto25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
