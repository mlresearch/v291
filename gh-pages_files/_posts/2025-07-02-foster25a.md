---
title: Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational
  Role of the Base Model in Exploration
section: Original Papers
abstract: 'Language model alignment (or, reinforcement learning) techniques that leverage
  active exploration – deliberately encouraging the model to produce diverse, informative
  responses – offer the promise of super-human capabilities. However, current understanding
  of algorithm design primitives for computationally efficient exploration with language
  models is limited. To better understand how to leverage access to powerful pre-trained
  generative models to improve the efficiency of exploration, we introduce a new computational
  framework for RL with language models, in which the learner interacts with the model
  through a sampling oracle. Focusing on the linear softmax model parameterization,
  we provide new results that reveal the computational-statistical tradeoffs of efficient
  exploration: 1. Necessity of coverage: Coverage refers to the extent to which the
  pre-trained model covers near-optimal responses – a form of hidden knowledge. We
  show that coverage, while not necessary for data efficiency, lower bounds the runtime
  of any algorithm in our framework. 2. Inference-time exploration: We introduce a
  new algorithm, SpannerSampling, which obtains optimal data efficiency and is computationally
  efficient whenever the pre-trained model enjoys sufficient coverage, matching our
  lower bound. SpannerSampling leverages inference-time computation with the pre-trained
  model to reduce the effective search space for exploration. 3. Insufficiency of
  training-time interventions: We contrast the result above by showing that training-time
  interventions that produce proper policies cannot achieve similar guarantees in
  polynomial time. 4. Computational benefits of multi-turn exploration: Finally, we
  show that under additional representational assumptions, one can achieve improved
  runtime (replacing sequence-level coverage with token-level coverage) through multi-turn
  exploration.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: foster25a
month: 0
tex_title: Is a Good Foundation Necessary for Efficient Reinforcement Learning? The
  Computational Role of the Base Model in Exploration
firstpage: 2026
lastpage: 2142
page: 2026-2142
order: 2026
cycles: false
bibtex_author: Foster, Dylan J and Mhammedi, Zakaria and Rohatgi, Dhruv
author:
- given: Dylan J
  family: Foster
- given: Zakaria
  family: Mhammedi
- given: Dhruv
  family: Rohatgi
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/foster25a/foster25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
