---
title: Quantifying Overfitting along the Regularization Path for Two-Part-Code MDL
  in Supervised Classification
section: Original Papers
abstract: We provide a complete characterization of the entire regularization curve
  of a modified two-part-code Minimum Description Length (MDL) learning rule for binary
  classification, based on an arbitrary prior or description language. Gr{ü}nwald
  and Langford (2004) previously established the lack of asymptotic consistency, from
  an agnostic PAC (frequentist worst case) perspective, of the MDL rule with a penalty
  parameter of $\lambda=1$, suggesting that it underegularizes. Driven by interest
  in understanding how benign or catastrophic under-regularization and overfitting
  might be, we obtain a precise quantitative description of the worst case limiting
  error as a function of the regularization parameter $\lambda$ and noise level (or
  approximation error), significantly tightening the analysis of Gr{ü}nwald and Langford
  for $\lambda=1$ and extending it to all other choices of $\lambda$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu25a
month: 0
tex_title: Quantifying Overfitting along the Regularization Path for Two-Part-Code
  {MDL} in Supervised Classification
firstpage: 6124
lastpage: 6155
page: 6124-6155
order: 6124
cycles: false
bibtex_author: Zhu, Xiaohan and Srebro, Nathan
author:
- given: Xiaohan
  family: Zhu
- given: Nathan
  family: Srebro
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/zhu25a/zhu25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
