---
title: 'Provable Complexity Improvement of AdaGrad over SGD: Upper and Lower Bounds
  in Stochastic Non-Convex Optimization'
section: Original Papers
abstract: Adaptive gradient methods, such as AdaGrad, are among the most successful
  optimization algorithms for neural network training. While these methods are known
  to achieve better dimensional dependence than stochastic gradient descent (SGD)
  for stochastic convex optimization under favorable geometry, the theoretical justification
  for their success in stochastic non-convex optimization remains elusive. In fact,
  under standard assumptions of Lipschitz gradients and bounded noise variance, it
  is known that SGD is worst-case optimal (up to absolute constants) in terms of finding
  a near-stationary point with respect to the $\ell_2$-norm, making further improvements
  impossible. Motivated by this limitation, we introduce refined assumptions on the
  smoothness structure of the objective and the gradient noise variance, which better
  suit the coordinate-wise nature of adaptive gradient methods. Moreover, we adopt
  the $\ell_1$-norm of the gradient as the stationarity measure, as opposed to the
  standard $\ell_2$-norm, to align with the coordinate-wise analysis and obtain tighter
  convergence guarantees for AdaGrad. Under these new assumptions and the $\ell_1$-norm
  stationarity measure, we establish an \emph{upper bound} on the convergence rate
  of AdaGrad and a corresponding \emph{lower bound} for SGD. In particular, we identify
  non-convex settings in which the iteration complexity of AdaGrad is favorable over
  SGD and show that, for certain configurations of problem parameters, it outperforms
  SGD by a factor of $d$, where $d$ is the problem dimension. To the best of our knowledge,
  this is the first result to demonstrate a provable gain of adaptive gradient methods
  over SGD in a non-convex setting. We also present supporting lower bounds, including
  one specific to AdaGrad and one applicable to general deterministic first-order
  methods, showing that our upper bound for AdaGrad is tight and unimprovable up to
  a logarithmic factor under certain conditions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jiang25c
month: 0
tex_title: 'Provable Complexity Improvement of AdaGrad over SGD: Upper and Lower Bounds
  in Stochastic Non-Convex Optimization'
firstpage: 3124
lastpage: 3158
page: 3124-3158
order: 3124
cycles: false
bibtex_author: Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan
author:
- given: Ruichen
  family: Jiang
- given: Devyani
  family: Maladkar
- given: Aryan
  family: Mokhtari
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/jiang25c/jiang25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
