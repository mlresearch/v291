---
title: The Adaptive Complexity of Finding a Stationary Point
section: Original Papers
abstract: In large-scale applications, such as machine learning, it is desirable to
  design non-convex optimization algorithms with a high degree of parallelization.
  In this work, we study the adaptive complexity of finding a stationary point, which
  is the minimal number of sequential rounds required to achieve stationarity given
  polynomially many queries executed in parallel at each round.  For the high-dimensional
  case, \emph{i.e.}, $d = \widetilde{\Omega}(\varepsilon^{-(2 + 2p)/p})$, we show
  that for any (potentially randomized) algorithm, there exists a function with Lipschitz
  $p$-th order derivatives such that the algorithm requires at least $\varepsilon^{-(p+1)/p}$
  iterations to find an $\varepsilon$-stationary point.  Our lower bounds are tight
  and show that even with $\mathrm{poly}(d)$  queries per iteration, no algorithm
  has better convergence rate than those achievable with one-query-per-round algorithms.
  In other words, gradient descent, the cubic-regularized Newtonâ€™s method, and the
  $p$-th order adaptive regularization method are adaptively optimal. Our proof relies
  upon novel analysis with the characterization of the output for the hardness potentials
  based on a chain-like structure with random partition. For the constant-dimensional
  case, \emph{i.e.}, $d = \Theta(1)$, we propose an algorithm that bridges grid search
  and gradient flow trapping, finding an approximate stationary point in constant
  iterations. Its asymptotic tightness is verified by a new lower bound on the required
  queries per iteration. We show there exists a smooth function such that any algorithm
  running with $\Theta(\log (1/\varepsilon))$ rounds requires at least $\widetilde{\Omega}((1/\varepsilon)^{(d-1)/2})$
  queries per round. This lower bound is tight up to a logarithmic factor, and implies
  that the gradient flow trapping is adaptively optimal.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huanjian25a
month: 0
tex_title: The Adaptive Complexity of Finding a Stationary Point
firstpage: 6091
lastpage: 6123
page: 6091-6123
order: 6091
cycles: false
bibtex_author: Huanjian, Zhou and Andi, Han and Akiko, Takeda and Masashi, Sugiyama
author:
- given: Zhou
  family: Huanjian
- given: Han
  family: Andi
- given: Takeda
  family: Akiko
- given: Sugiyama
  family: Masashi
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/huanjian25a/huanjian25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
