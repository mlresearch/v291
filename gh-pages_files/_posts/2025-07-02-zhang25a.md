---
title: Anytime Acceleration of Gradient Descent
section: Original Papers
abstract: 'This work investigates stepsize-based acceleration of gradient descent
  with anytime convergence guarantees. For smooth (non-strongly) convex optimization,
  we propose a stepsize schedule that allows gradient descent to achieve convergence
  guarantees of $O\big(T^{-\frac{2\log_2\rho}{1+\log_2\rho}}\big) \approx O(T^{-1.119})$
  for any stopping time  $T$, where $\rho=\sqrt{2}+1$ is the silver ratio and the
  stepsize schedule is predetermined without prior knowledge of the stopping time.
  This result provides an affirmative answer to a COLT open problem  regarding whether
  stepsize-based acceleration  can yield anytime convergence rates of $o(T^{-1})$.
  We further extend our theory to yield anytime convergence guarantees of $\exp(-\Omega(T/\kappa^{0.893}))$
  for smooth and strongly convex optimization, with $\kappa$ being the condition number. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang25a
month: 0
tex_title: Anytime Acceleration of Gradient Descent
firstpage: 5991
lastpage: 6013
page: 5991-6013
order: 5991
cycles: false
bibtex_author: Zhang, Zihan and Lee, Jason and Du, Simon and Chen, Yuxin
author:
- given: Zihan
  family: Zhang
- given: Jason
  family: Lee
- given: Simon
  family: Du
- given: Yuxin
  family: Chen
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/zhang25a/zhang25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
