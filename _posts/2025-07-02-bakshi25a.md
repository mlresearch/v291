---
title: Metric Embeddings Beyond Bi-Lipschitz Distortion via Sherali-Adams
section: Original Papers
abstract: ' Metric embeddings are a widely used method in algorithm design, where
  generally a "complex" metric is embedded into a simpler, lower-dimensional one.
  Historically, the theoretical computer science community has focused on bi-Lipschitz
  embeddings, which guarantee that every pairwise distance is approximately preserved.
  In contrast, alternative embedding objectives that are commonly used in practice
  avoid bi-Lipschitz distortion; yet these approaches have received comparatively
  less study in theory. In this paper, we focus on Multi-dimensional Scaling (MDS),
  where we are given a set of non-negative dissimilarities $\{d_{i,j}\}_{i,j\in[n]}$
  over $n$ points, and the goal is to find an embedding $\{x_1,…,x_n\}\subset\mathbb{R}^k$
  that minimizes \[ \mathrm{OPT} = \min_{x_1,…,x_n} \mathbb{E}_{i,j\in[n]} \left[
  \left(1-\frac{\|x_i - x_j\|}{d_{i,j}}\right)^2 \right]. \]{Despite} its popularity,
  our theoretical understanding of MDS is extremely limited. Recently, Demaine et
  al. gave the first approximation algorithm with provable guarantees for this objective,
  which achieves an embedding in constant-dimensional Euclidean space with cost $\mathrm{OPT}
  + \epsilon$ in $n^2 \cdot 2^{\mathrm{poly}(\Delta/\epsilon)}$ time, where $\Delta$
  is the aspect ratio of the input dissimilarities. For metrics that admit low-cost
  embeddings, $\Delta$ scales polynomially in $n$. In this work, we give the first
  approximation algorithm for MDS with quasi-polynomial dependency on $\Delta$: for
  constant-dimensional Euclidean space, we achieve a solution with cost $O(\log \Delta)\cdot
  \mathrm{OPT}^{\Omega(1)} + \epsilon$ in time $n^{O(1)} \cdot 2^{\mathrm{poly}\left(\frac{\log(\Delta)}{\epsilon}\right)}$.
  Our algorithms are based on a novel geometry-aware analysis of a conditional rounding
  of the Sherali-Adams LP hierarchy, allowing us to avoid the exponential dependency
  on the aspect ratio that would typically result from this rounding. \end{abstract} '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bakshi25a
month: 0
tex_title: Metric Embeddings Beyond Bi-Lipschitz Distortion via Sherali-Adams
firstpage: 264
lastpage: 279
page: 264-279
order: 264
cycles: false
bibtex_author: Bakshi, Ainesh and Cohen-Addad, Vincent and Jayaram, Rajesh and Hopkins,
  Samuel B. and Lattanzi, Silvio
author:
- given: Ainesh
  family: Bakshi
- given: Vincent
  family: Cohen-Addad
- given: Rajesh
  family: Jayaram
- given: Samuel B.
  family: Hopkins
- given: Silvio
  family: Lattanzi
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/bakshi25a/bakshi25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
