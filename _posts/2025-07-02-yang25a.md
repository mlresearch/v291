---
title: Fast and Multiphase Rates for Nearest Neighbor Classifiers
section: Original Papers
abstract: 'We study the scaling of classification error rates with respect to the
  size of the training dataset. In contrast to classical results where rates are minimax
  optimal for a problem class, this work starts with the empirical observation that,
  even for a fixed data distribution,  the error scaling can have \emph{diverse} rates
  across different ranges of sample size.  To understand when and why the error rate
  is non-uniform, we theoretically analyze nearest neighbor classifiers. We show that
  an error scaling law can have fine-grained rates: in the early phase, the test error
  depends polynomially on the data dimension and decreases fast; whereas in the later
  phase, the error depends exponentially on the data dimension and decreases slowly.  Our
  analysis highlights the complexity of the data distribution in determining the test
  error. When the data are distributed benignly, we show that the generalization error
  of nearest neighbor classifier can depend polynomially, instead of exponentially,
  on the data dimension. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang25a
month: 0
tex_title: Fast and Multiphase Rates for Nearest Neighbor Classifiers
firstpage: 6014
lastpage: 6015
page: 6014-6015
order: 6014
cycles: false
bibtex_author: Yang, Pengkun and Zhang, Jingzhao
author:
- given: Pengkun
  family: Yang
- given: Jingzhao
  family: Zhang
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/yang25a/yang25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
