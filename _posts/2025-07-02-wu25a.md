---
title: 'Taking a Big Step: Large Learning Rates in Denoising Score Matching Prevent
  Memorization'
section: Original Papers
abstract: Denoising score matching plays a pivotal role in the performance of diffusion-based
  generative models. However, the empirical optimal score–the exact solution to the
  denoising score matching–leads to memorization, where generated samples replicate
  the training data. Yet, in practice, only a moderate degree of memorization is observed,
  even without explicit regularization. In this paper, we investigate this phenomenon
  by uncovering an implicit regularization mechanism driven by large learning rates.
  Specifically, we show that in the small-noise regime, the empirical optimal score
  exhibits high irregularity. We then prove that, when trained by stochastic gradient
  descent with a large enough learning rate, neural networks cannot stably converge
  to a local minimum with arbitrarily small excess risk. Consequently, the learned
  score cannot be arbitrarily close to the empirical optimal score, thereby mitigating
  memorization. To make the analysis tractable, we consider one-dimensional data and
  two-layer neural networks. Experiments validate the crucial role of the learning
  rate in preventing memorization, even beyond the one-dimensional setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu25a
month: 0
tex_title: 'Taking a Big Step: Large Learning Rates in Denoising Score Matching Prevent
  Memorization'
firstpage: 5718
lastpage: 5756
page: 5718-5756
order: 5718
cycles: false
bibtex_author: Wu, Yu-Han and Marion, Pierre and Biau, G\'erard and Boyer, Claire
author:
- given: Yu-Han
  family: Wu
- given: Pierre
  family: Marion
- given: Gérard
  family: Biau
- given: Claire
  family: Boyer
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/wu25a/wu25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
