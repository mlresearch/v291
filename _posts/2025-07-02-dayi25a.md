---
title: Low-rank fine-tuning lies between lazy training and feature learning
section: Original Papers
abstract: LoRA has emerged as one of the de facto methods for fine-tuning foundation
  models with low computational cost and memory footprint. The idea is to only train
  a low-rank perturbation to the weights of a pre-trained model, given supervised
  data for a downstream task. Despite its empirical success, mathematically it remains
  poorly understood what learning mechanisms ensure that gradient descent converges
  to useful low-rank perturbations. In this work we study low-rank fine-tuning in
  a student-teacher setting. We are given the weights of a two-layer base model $f$,
  as well as i.i.d. samples $(x,f^*(x))$ where $x$ is Gaussian and $f^*$ is the teacher
  model given by perturbing the weights of $f$ by a rank-1 matrix. This generalizes
  the setting of generalized linear model (GLM) regression where the weights of $f$
  are zero. When the rank-1 perturbation is comparable in norm to the weight matrix
  of $f$, we show that the training dynamics are genuinely distinct from both the
  lazy linearized dynamics of the kernel regime, and the rich feature learning dynamics
  captured by GLM regression. We prove under mild assumptions that a student model
  which is initialized at the base model and trained with online SGD will converge
  to the teacher in $dk^{O(1)}$ iterations, where $k$ is the number of neurons in
  $f$. Importantly, unlike in the GLM setting, the complexity does not depend on fine-grained
  properties of the activation’s Hermite expansion. We also prove that in our setting,
  learning the teacher model “from scratch” can require significantly more iterations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dayi25a
month: 0
tex_title: Low-rank fine-tuning lies between lazy training and feature learning
firstpage: 1415
lastpage: 1471
page: 1415-1471
order: 1415
cycles: false
bibtex_author: Dayi, Arif Kerem and Chen, Sitan
author:
- given: Arif Kerem
  family: Dayi
- given: Sitan
  family: Chen
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/dayi25a/dayi25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
