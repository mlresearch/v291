---
title: An uncertainty principle for Linear Recurrent Neural Networks
section: Original Papers
abstract: " We consider linear recurrent neural networks, which have become a key
  building block of sequence modeling due to their ability for stable and effective
  long-range modeling. In this paper, we aim at characterizing this ability on the
  simple but core copy task, whose goal is to build a linear filter of order $S$ that
  approximates the filter that looks $K$ time steps in the past (which we refer to
  as the shift-$K$ filter), where $K$ is  larger than $S$. Using classical signal
  models and quadratic cost, we fully characterize the problem by providing lower
  bounds of approximation, as well as explicit filters that achieve this lower bound
  up to constants. The optimal performance highlights an uncertainty principle for
  this task: the optimal filter has to average values around the $K$-th time step
  in the past with a range (width) that is proportional to $K/S$."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: francois25a
month: 0
tex_title: An uncertainty principle for Linear Recurrent Neural Networks
firstpage: 2143
lastpage: 2187
page: 2143-2187
order: 2143
cycles: false
bibtex_author: Fran\c{c}ois, Alexandre and Orvieto, Antonio and Bach, Francis
author:
- given: Alexandre
  family: François
- given: Antonio
  family: Orvieto
- given: Francis
  family: Bach
date: 2025-07-02
address:
container-title: Proceedings of Thirty Eighth Conference on Learning Theory
volume: '291'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v291/main/assets/francois25a/francois25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
